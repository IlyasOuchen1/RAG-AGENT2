import streamlit as st
import os
from dotenv import load_dotenv
import tempfile

# LangChain imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Pinecone
from pinecone import Pinecone, ServerlessSpec

# Configuration Streamlit
st.set_page_config(page_title="RAG Assistant", layout="wide")

# Variables d'environnement
load_dotenv()

# V√©rification des cl√©s API
if not os.getenv("OPENAI_API_KEY") or not os.getenv("PINECONE_API_KEY"):
    st.error("‚ùå Cl√©s API manquantes. V√©rifiez votre fichier .env")
    st.stop()

# Configuration Pinecone (corrig√©e pour le plan gratuit)
@st.cache_resource
def init_pinecone():
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index_name = "simple-rag"
    
    # Cr√©er l'index s'il n'existe pas (r√©gion compatible avec le plan gratuit)
    existing_indexes = [index.name for index in pc.list_indexes()]
    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")  # R√©gion compatible plan gratuit
        )
        st.success("‚úÖ Index Pinecone cr√©√© avec succ√®s!")
    
    return pc.Index(index_name)

# Configuration du vector store
@st.cache_resource
def init_vector_store():
    index = init_pinecone()
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    return PineconeVectorStore(index=index, embedding=embeddings)

# Configuration du LLM avec GPT-4o Mini (version √©quilibr√©e)
@st.cache_resource
def init_llm():
    return ChatOpenAI(
        model="gpt-4o-mini",  # GPT-4o Mini - √©quilibre vitesse/qualit√©
        temperature=0.1,      # L√©g√®re cr√©ativit√© contr√¥l√©e
        max_tokens=2000,      # Limite g√©n√©reuse pour des r√©ponses compl√®tes
        request_timeout=30    # Timeout standard
    )

# Outils de recherche avanc√©s
vector_store = init_vector_store()

# Configuration Wikipedia
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000))

@tool
def search_wikipedia(query: str) -> str:
    """Recherche des informations sur Wikipedia."""
    try:
        result = wikipedia.run(query)
        if result:
            response = f"üìö **R√©sultats Wikipedia pour '{query}':**\n\n{result}\n\nSource: Wikipedia"
            response += "\n\nüìö **Sources utilis√©es:**\n‚Ä¢ Wikipedia"
            return response
        else:
            return f"Aucun r√©sultat trouv√© sur Wikipedia pour '{query}'."
    except Exception as e:
        return f"Erreur lors de la recherche Wikipedia: {str(e)}"

@tool
def retrieve(query: str) -> str:
    """Outil principal de retrieve - recherche dans tous les documents."""
    docs = vector_store.similarity_search(query, k=3)
    if not docs:
        return "Aucune information trouv√©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"üìÑ {filename} - R√©sultat {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nüìö **Sources utilis√©es:**")
    for source in sorted(sources):
        results.append(f"‚Ä¢ {source}")
    
    return "\n".join(results)

@tool
def search_documents(query: str) -> str:
    """Recherche g√©n√©rale dans les documents upload√©s."""
    docs = vector_store.similarity_search(query, k=3)
    if not docs:
        return "Aucune information trouv√©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"üìÑ {filename} - Extrait {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nüìö **Sources utilis√©es:**")
    for source in sorted(sources):
        results.append(f"‚Ä¢ {source}")
    
    return "\n".join(results)

@tool
def search_specific_document(filename: str, query: str) -> str:
    """Recherche dans un document sp√©cifique par nom de fichier."""
    docs = vector_store.similarity_search(
        query, 
        k=5,
        filter={"filename": filename}
    )
    if not docs:
        return f"Aucune information trouv√©e dans le document '{filename}' pour cette requ√™te."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"üìÑ {filename} - Section {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nüìö **Sources utilis√©es:**")
    for source in sorted(sources):
        results.append(f"‚Ä¢ {source}")
    
    return "\n".join(results)

@tool
def get_document_summary(filename: str) -> str:
    """Obtient un r√©sum√© d'un document sp√©cifique."""
    docs = vector_store.similarity_search(
        "", 
        k=10,  # Plus de chunks pour un meilleur r√©sum√©
        filter={"filename": filename}
    )
    if not docs:
        return f"Document '{filename}' non trouv√©."
    
    content = "\n".join([doc.page_content for doc in docs[:5]])  # Premiers 5 chunks
    return f"üìÑ R√©sum√© de '{filename}':\n{content[:1000]}..."

@tool
def get_database_stats() -> str:
    """Obtient des statistiques sur la base de donn√©es documentaire."""
    try:
        # √âchantillonnage pour obtenir des stats
        sample_docs = vector_store.similarity_search("", k=100)
        
        if not sample_docs:
            return "La base de donn√©es est vide."
        
        # Compter les documents uniques
        filenames = [doc.metadata.get('filename', 'Inconnu') for doc in sample_docs]
        unique_files = set(filenames)
        
        # Compter les chunks par document
        file_chunks = {}
        for filename in filenames:
            file_chunks[filename] = file_chunks.get(filename, 0) + 1
        
        # Cr√©er le rapport
        stats = f"üìä **Statistiques de la base de donn√©es:**\n\n"
        stats += f"‚Ä¢ Nombre total de chunks: {len(sample_docs)}\n"
        stats += f"‚Ä¢ Nombre de documents: {len(unique_files)}\n\n"
        
        stats += "üìÑ **D√©tails par document:**\n"
        for filename in sorted(unique_files):
            chunk_count = file_chunks.get(filename, 0)
            stats += f"‚Ä¢ {filename}: {chunk_count} chunks\n"
        
        return stats
    except Exception as e:
        return f"Erreur lors de la r√©cup√©ration des statistiques: {str(e)}"

@tool
def list_available_documents() -> str:
    """Liste tous les documents disponibles dans la base avec d√©tails."""
    try:
        # R√©cup√©rer un √©chantillon de documents pour obtenir les noms de fichiers
        docs = vector_store.similarity_search("", k=50)
        
        if not docs:
            return "Aucun document n'a √©t√© upload√©."
        
        # Organiser par nom de fichier avec m√©tadonn√©es
        files_info = {}
        for doc in docs:
            filename = doc.metadata.get('filename', 'Inconnu')
            if filename not in files_info:
                files_info[filename] = {
                    'chunks': 0,
                    'upload_time': doc.metadata.get('upload_time', 'N/A'),
                    'file_size': doc.metadata.get('file_size', 'N/A')
                }
            files_info[filename]['chunks'] += 1
        
        # Formater la liste
        result = "üìö **Documents disponibles:**\n\n"
        for filename in sorted(files_info.keys()):
            info = files_info[filename]
            result += f"üìÑ **{filename}**\n"
            result += f"   ‚Ä¢ Chunks: {info['chunks']}\n"
            if info['upload_time'] != 'N/A':
                # Formatage simplifi√© de la date
                try:
                    date_part = info['upload_time'].split('T')[0]
                    result += f"   ‚Ä¢ Ajout√© le: {date_part}\n"
                except:
                    pass
            result += "\n"
        
        return result
    except Exception as e:
        return f"Erreur: {str(e)}"

@tool
def search_with_context(query: str, context_size: int = 5) -> str:
    """Recherche avec plus de contexte (plus de chunks)."""
    docs = vector_store.similarity_search(query, k=context_size)
    if not docs:
        return "Aucune information trouv√©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"üìÑ {filename} - Contexte {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nüìö **Sources utilis√©es:**")
    for source in sorted(sources):
        results.append(f"‚Ä¢ {source}")
    
    return "\n".join(results)

@tool
def find_related_content(query: str) -> str:
    """Trouve du contenu li√©/similaire dans les documents."""
    # Premi√®re recherche
    initial_docs = vector_store.similarity_search(query, k=2)
    if not initial_docs:
        return "Aucun contenu li√© trouv√©."
    
    # Utiliser le contenu trouv√© pour chercher du contenu similaire
    related_results = []
    sources = set()
    for doc in initial_docs:
        # Recherche bas√©e sur le contenu trouv√©
        related_docs = vector_store.similarity_search(doc.page_content[:200], k=2)
        for related_doc in related_docs:
            if related_doc.page_content != doc.page_content:  # √âviter les doublons
                filename = related_doc.metadata.get('filename', 'Document inconnu')
                sources.add(filename)
                source = f"Source: {filename}"
                related_results.append(f"üìÑ {filename} - Contenu li√©:\n{related_doc.page_content}\n{source}\n")
    
    if not related_results:
        return "Aucun contenu li√© suppl√©mentaire trouv√©."
    
    # Add sources summary
    related_results.append("\nüìö **Sources utilis√©es:**")
    for source in sorted(sources):
        related_results.append(f"‚Ä¢ {source}")
    
    return "\n".join(related_results[:3])  # Limiter √† 3 r√©sultats

# Configuration de l'agent avec tous les outils
def create_agent(search_mode="both"):
    llm = init_llm()
    
    # Define tools based on search mode
    tools = []
    if search_mode in ["both", "internal"]:
        tools.extend([
            retrieve,  # Outil principal de retrieve
            search_documents,
            search_specific_document,
            get_document_summary,
            list_available_documents,
            search_with_context,
            find_related_content
        ])
    if search_mode in ["both", "external"]:
        tools.append(search_wikipedia)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es un assistant expert en recherche documentaire. Tu as acc√®s √† plusieurs outils puissants :

üîç **OUTILS DISPONIBLES** :
1. **retrieve(query)** : Outil principal pour r√©cup√©rer des informations
2. **search_documents(query)** : Recherche g√©n√©rale dans tous les documents
3. **search_specific_document(filename, query)** : Recherche dans un document sp√©cifique
4. **get_document_summary(filename)** : R√©sum√© d'un document
5. **list_available_documents()** : Liste tous les documents disponibles
6. **search_with_context(query, context_size)** : Recherche avec plus de contexte
7. **find_related_content(query)** : Trouve du contenu connexe

üìã **INSTRUCTIONS** :
- Utilise TOUJOURS au moins un outil avant de r√©pondre
- Pour les questions g√©n√©rales ‚Üí retrieve() ou search_documents()
- Pour chercher dans un fichier sp√©cifique ‚Üí search_specific_document()
- Pour lister les fichiers ‚Üí list_available_documents()
- Pour plus de d√©tails ‚Üí search_with_context()
- R√©ponds UNIQUEMENT avec les informations trouv√©es

üí° **EXEMPLES D'USAGE** :
- "Dis-moi ce que contiennent les documents" ‚Üí retrieve("contenu")
- "R√©sume le document rapport.pdf" ‚Üí get_document_summary("rapport.pdf")
- "Cherche dans document.pdf les infos sur X" ‚Üí search_specific_document("document.pdf", "X")

üö´ **R√àGLES STRICTES** :
- Jamais de connaissances g√©n√©rales
- Toujours utiliser les outils
- Si rien trouv√© : "Aucune information sur ce sujet dans les documents."
"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    agent = create_tool_calling_agent(llm, tools, prompt)
    return AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        max_iterations=3,
        handle_parsing_errors=True
    )

# Fonction pour v√©rifier si un document existe d√©j√†
def check_document_exists(filename):
    """V√©rifie si un document est d√©j√† dans la base vectorielle."""
    try:
        # Rechercher des documents avec ce nom de fichier
        docs = vector_store.similarity_search(
            "", 
            k=10,
            filter={"filename": filename}
        )
        return len(docs) > 0
    except Exception as e:
        return False

# Fonction pour traiter les fichiers (avec gestion des gros fichiers)
def process_file(uploaded_file):
    """Traite et indexe un fichier upload√© (√©vite les doublons et g√®re les gros fichiers)."""
    
    # V√©rifier si le document existe d√©j√†
    if check_document_exists(uploaded_file.name):
        return "exists"
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    # Charger le document
    if uploaded_file.name.endswith('.pdf'):
        loader = PyPDFLoader(tmp_path)
    elif uploaded_file.name.endswith('.txt'):
        loader = TextLoader(tmp_path)
    elif uploaded_file.name.endswith('.docx'):
        loader = Docx2txtLoader(tmp_path)
    else:
        os.unlink(tmp_path)
        return False
    
    docs = loader.load()
    os.unlink(tmp_path)
    
    # Diviser en chunks √©quilibr√©s
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # Taille √©quilibr√©e
        chunk_overlap=100,   # Bon overlap pour la coh√©rence
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(docs)
    
    # Ajouter metadata avec timestamp pour tra√ßabilit√©
    import datetime
    current_time = datetime.datetime.now().isoformat()
    
    for i, chunk in enumerate(chunks):
        chunk.metadata.update({
            'filename': uploaded_file.name,
            'chunk_index': i,
            'total_chunks': len(chunks),
            'upload_time': current_time,
            'file_size': len(uploaded_file.getvalue())
        })
    
    # Indexer par batches pour √©viter les erreurs de taille
    batch_size = 50  # Traiter 50 chunks √† la fois
    total_batches = (len(chunks) + batch_size - 1) // batch_size
    
    try:
        progress_text = st.empty()
        progress_bar = st.progress(0)
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            current_batch = (i // batch_size) + 1
            
            # Mettre √† jour l'interface
            progress_text.text(f"‚è≥ Indexation batch {current_batch}/{total_batches} ({len(batch)} chunks)")
            progress_bar.progress(current_batch / total_batches)
            
            # Ajouter le batch √† Pinecone
            vector_store.add_documents(batch)
            
            # Pause courte pour √©viter la surcharge
            import time
            time.sleep(0.5)
        
        # Nettoyer l'interface
        progress_text.empty()
        progress_bar.empty()
        
        return len(chunks)
        
    except Exception as e:
        st.error(f"Erreur lors de l'indexation: {str(e)}")
        return False

# Interface utilisateur am√©lior√©e
st.title("ü§ñ Assistant RAG Multilingue (GPT-4o Mini)")
st.caption("üåê Posez vos questions en fran√ßais, anglais, espagnol ou toute autre langue")
st.caption("‚ö° Powered by GPT-4o Mini - Rapide et efficace")

# Sidebar pour upload avec gestion des doublons
with st.sidebar:
    st.header("üìÅ Upload de Documents")
    
    # Afficher le statut de la base de donn√©es avec gestion des erreurs
    try:
        # Obtenir quelques stats sur les documents existants
        sample_docs = vector_store.similarity_search("", k=20)
        existing_files = set([doc.metadata.get('filename', 'Inconnu') for doc in sample_docs])
        
        if existing_files:
            st.success(f"‚úÖ Connexion Pinecone active")
            st.info(f"üìö {len(existing_files)} document(s) d√©j√† dans la base")
            with st.expander("Voir les documents existants"):
                for filename in sorted(existing_files):
                    st.write(f"üìÑ {filename}")
        else:
            st.info("üìÇ Base de donn√©es vide - Pr√™te pour le premier document")
    except Exception as e:
        st.error(f"‚ö†Ô∏è Probl√®me de connexion Pinecone: {str(e)}")
        st.info("üí° V√©rifiez votre cl√© API et la configuration de l'index")
    
    uploaded_file = st.file_uploader(
        "Choisir un fichier",
        type=['pdf', 'txt', 'docx']
    )
    
    if uploaded_file:
        # Afficher des informations sur le fichier
        file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.write(f"üìÅ Fichier: {uploaded_file.name}")
        st.write(f"üìè Taille: {file_size_mb:.2f} MB")
        
        if file_size_mb > 5:
            st.warning("‚ö†Ô∏è Fichier volumineux - Le traitement peut prendre du temps")
        
        with st.spinner("‚è≥ V√©rification et traitement..."):
            result = process_file(uploaded_file)
            
            if result == "exists":
                st.warning(f"‚ö†Ô∏è Le document '{uploaded_file.name}' existe d√©j√† dans la base !")
                st.info("üí° Pas besoin de le reprocesser. Vous pouvez directement poser des questions.")
            elif result and isinstance(result, int):
                st.success(f"‚úÖ Nouveau document ajout√© ! {result} chunks index√©s pour '{uploaded_file.name}'")
                st.balloons()  # Animation de c√©l√©bration
            elif result == False:
                st.error("‚ùå Type de fichier non support√© ou erreur de traitement")
            else:
                st.error("‚ùå Erreur lors du traitement")
    
    # Bouton pour effacer la base (optionnel)
    st.divider()
    if st.button("üóëÔ∏è Effacer la base de donn√©es", type="secondary"):
        if st.checkbox("Confirmer la suppression"):
            try:
                # Ici on peut supprimer l'index et le recr√©er
                # Ou utiliser d'autres m√©thodes selon votre configuration Pinecone
                st.warning("‚ö†Ô∏è Fonctionnalit√© de suppression √† impl√©menter si n√©cessaire")
            except Exception as e:
                st.error(f"Erreur: {e}")
    
    # Section d'aide
    st.header("üí° Exemples de Questions")
    st.markdown("""
    **üåê L'assistant r√©pond dans votre langue :**
    
    **üìÑ Recherche Interne (Documents):**
    - "Quels documents sont disponibles ?"
    - "What documents are available?"
    - "R√©sume le document rapport.pdf"
    - "Search for information about [topic] in my documents"
    
    **üåê Recherche Externe (Wikipedia):**
    - "Recherche sur Wikipedia l'intelligence artificielle"
    - "Search Wikipedia for machine learning"
    - "¬øQu√© dice Wikipedia sobre blockchain?"
    - "Find information about quantum computing on Wikipedia"
    
    **üîÑ Recherche Combin√©e (Les Deux):**
    - "Compare mes documents avec les infos Wikipedia sur [sujet]"
    - "What do my documents say about AI vs Wikipedia?"
    - "Informaci√≥n interna y externa sobre [tema]"
    
    **üîç Recherche avanc√©e :**
    - "Search with more context on [topic]"
    - "Cherche du contenu li√© √† [sujet]"
    - "Statistiques de la base de donn√©es"
    """)
    
    # Indicateur du mode actuel
    current_mode = st.session_state.get("search_mode", "both")
    if current_mode == "both":
        st.success("üîÑ Mode Hybride: Documents + Wikipedia")
    elif current_mode == "internal":
        st.info("üìÑ Mode Interne: Documents uniquement")
    else:
        st.info("üåê Mode Externe: Wikipedia uniquement")

# Zone principale avec s√©lecteur de mode
col1, col2 = st.columns([3, 1])

with col2:
    st.subheader("üîç Mode de Recherche")
    # Initialiser search_mode dans session_state s'il n'existe pas
    if "search_mode" not in st.session_state:
        st.session_state.search_mode = "both"
    
    search_mode = st.radio(
        "Choisissez le type de recherche:",
        ["both", "internal", "external"],
        format_func=lambda x: {
            "both": "üîÑ Les Deux (Documents + Wikipedia)",
            "internal": "üìÑ Documents Upload√©s Seulement", 
            "external": "üåê Wikipedia Seulement"
        }[x],
        index=0,
        key="search_mode_radio"
    )
    
    # Mettre √† jour session_state
    st.session_state.search_mode = search_mode
    
    st.subheader("üõ†Ô∏è Outils Disponibles")
    current_mode = st.session_state.get("search_mode", "both")
    if current_mode in ["internal", "both"]:
        st.write("""
        **üìÑ Outils Internes:**
        - üîç Recherche g√©n√©rale
        - üìÑ Recherche sp√©cifique
        - üìã R√©sum√© de document
        - üìö Liste des documents
        - üîé Recherche contextuelle
        - üîó Contenu connexe
        - üìä Statistiques
        """)
    
    if current_mode in ["external", "both"]:
        st.write("""
        **üåê Outils Externes:**
        - üìö Recherche Wikipedia
        """)
    
    st.info(f"Mode actuel: {current_mode.upper()}")
    st.caption("üåê L'assistant s'adapte automatiquement √† la langue de votre question")

with col1:
    # Chat interface
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Afficher l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # Input utilisateur avec traitement direct de la langue
    if prompt := st.chat_input("Posez votre question... | Ask your question..."):
        # Ajouter le message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # D√©tecter la langue et cr√©er une instruction explicite
        def is_english(text):
            english_words = ['what', 'how', 'where', 'when', 'why', 'can', 'could', 'would', 'should', 'list', 'show', 'tell', 'explain', 'describe', 'the', 'for', 'tools', 'models', 'used']
            return any(word.lower() in text.lower() for word in english_words)
        
        def is_french(text):
            french_words = ['que', 'quoi', 'comment', 'o√π', 'quand', 'pourquoi', 'peux', 'pourrais', 'devrait', 'liste', 'montre', 'dis', 'explique', 'd√©cris', 'les', 'pour', 'outils', 'mod√®les', 'utilis√©s']
            return any(word.lower() in text.lower() for word in french_words)
        
        # Cr√©er un prompt avec instruction de langue explicite
        if is_english(prompt):
            enhanced_prompt = f"INSTRUCTION: Respond in English only.\n\nUser question: {prompt}"
        elif is_french(prompt):
            enhanced_prompt = f"INSTRUCTION: R√©pondre en fran√ßais uniquement.\n\nQuestion utilisateur: {prompt}"
        else:
            enhanced_prompt = f"INSTRUCTION: Respond in the same language as this question.\n\nUser question: {prompt}"
        
        # G√©n√©rer la r√©ponse avec le mode s√©lectionn√©
        current_search_mode = st.session_state.get("search_mode", "both")
        agent = create_agent(search_mode=current_search_mode)
        with st.chat_message("assistant"):
            with st.spinner("üîç Analyse en cours... | Analysis in progress..."):
                try:
                    # Pr√©parer l'historique pour l'agent
                    chat_history = []
                    for msg in st.session_state.messages[:-1]:  # Exclure le dernier message
                        if msg["role"] == "user":
                            chat_history.append(HumanMessage(content=msg["content"]))
                        else:
                            chat_history.append(AIMessage(content=msg["content"]))
                    
                    # Ajouter l'information du mode de recherche au prompt
                    mode_info = {
                        "both": "You can search in uploaded documents AND Wikipedia. Use both sources when relevant.",
                        "internal": "You can ONLY search in uploaded documents. Do not use external knowledge.",
                        "external": "You can ONLY search on Wikipedia. Do not refer to uploaded documents."
                    }
                    
                    enhanced_prompt = f"{enhanced_prompt}\n\nSEARCH MODE: {current_search_mode.upper()} - {mode_info[current_search_mode]}"
                    
                    # Ex√©cuter l'agent avec l'instruction de langue et mode
                    response = agent.invoke({
                        "input": enhanced_prompt,
                        "chat_history": chat_history
                    })
                    
                    answer = response['output']
                    
                    # Post-traitement pour v√©rifier la langue (en cas d'√©chec)
                    if is_english(prompt) and not any(word in answer.lower() for word in ['the', 'are', 'is', 'and', 'tools', 'documents']):
                        # Si la r√©ponse semble √™tre en fran√ßais alors que la question √©tait en anglais
                        st.warning("üîÑ Correction automatique de la langue...")
                        
                        # Nouvelle tentative avec instruction plus forte
                        second_prompt = f"You MUST respond in ENGLISH. Translate and reformat this answer to English: {answer}"
                        second_response = agent.invoke({
                            "input": second_prompt,
                            "chat_history": []
                        })
                        answer = second_response['output']
                    
                    st.write(answer)
                    
                    # Afficher les outils utilis√©s
                    if 'intermediate_steps' in response:
                        with st.expander("üîß Outils utilis√©s | Tools used"):
                            for step in response['intermediate_steps']:
                                tool_name = step[0].tool
                                tool_input = step[0].tool_input
                                st.write(f"**{tool_name}** : {tool_input}")
                    
                    # Ajouter √† l'historique
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    error_msg = f"Erreur | Error: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

# Bouton pour vider l'historique
if st.sidebar.button("üóëÔ∏è Vider l'historique"):
    st.session_state.messages = []
    st.rerun()