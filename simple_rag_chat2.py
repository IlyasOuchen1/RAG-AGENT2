import streamlit as st
import os
from dotenv import load_dotenv
import tempfile

# LangChain imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Pinecone
from pinecone import Pinecone, ServerlessSpec

# Configuration Streamlit
st.set_page_config(page_title="RAG Assistant", layout="wide")

# Variables d'environnement
load_dotenv()

# VÃ©rification des clÃ©s API
if not os.getenv("OPENAI_API_KEY") or not os.getenv("PINECONE_API_KEY"):
    st.error("âŒ ClÃ©s API manquantes. VÃ©rifiez votre fichier .env")
    st.stop()

# Configuration Pinecone (corrigÃ©e pour le plan gratuit)
@st.cache_resource
def init_pinecone():
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index_name = "simple-rag"
    
    # CrÃ©er l'index s'il n'existe pas (rÃ©gion compatible avec le plan gratuit)
    existing_indexes = [index.name for index in pc.list_indexes()]
    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")  # RÃ©gion compatible plan gratuit
        )
        st.success("âœ… Index Pinecone crÃ©Ã© avec succÃ¨s!")
    
    return pc.Index(index_name)

# Configuration du vector store
@st.cache_resource
def init_vector_store():
    index = init_pinecone()
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    return PineconeVectorStore(index=index, embedding=embeddings)

# Configuration du LLM avec GPT-4o Mini (version Ã©quilibrÃ©e)
@st.cache_resource
def init_llm():
    return ChatOpenAI(
        model="gpt-4o-mini",  # GPT-4o Mini - Ã©quilibre vitesse/qualitÃ©
        temperature=0.1,      # LÃ©gÃ¨re crÃ©ativitÃ© contrÃ´lÃ©e
        max_tokens=2000,      # Limite gÃ©nÃ©reuse pour des rÃ©ponses complÃ¨tes
        request_timeout=30    # Timeout standard
    )

# Outils de recherche avancÃ©s
vector_store = init_vector_store()

# Configuration Wikipedia
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000))

@tool
def search_wikipedia(query: str) -> str:
    """Recherche des informations sur Wikipedia."""
    try:
        result = wikipedia.run(query)
        if result:
            response = f"ğŸ“š **RÃ©sultats Wikipedia pour '{query}':**\n\n{result}\n\nSource: Wikipedia"
            response += "\n\nğŸ“š **Sources utilisÃ©es:**\nâ€¢ Wikipedia"
            return response
        else:
            return f"Aucun rÃ©sultat trouvÃ© sur Wikipedia pour '{query}'."
    except Exception as e:
        return f"Erreur lors de la recherche Wikipedia: {str(e)}"

@tool
def retrieve(query: str) -> str:
    """Outil principal de retrieve - recherche dans tous les documents."""
    docs = vector_store.similarity_search(query, k=3)
    if not docs:
        return "Aucune information trouvÃ©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"ğŸ“„ {filename} - RÃ©sultat {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nğŸ“š **Sources utilisÃ©es:**")
    for source in sorted(sources):
        results.append(f"â€¢ {source}")
    
    return "\n".join(results)

@tool
def search_documents(query: str) -> str:
    """Recherche gÃ©nÃ©rale dans les documents uploadÃ©s."""
    docs = vector_store.similarity_search(query, k=3)
    if not docs:
        return "Aucune information trouvÃ©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"ğŸ“„ {filename} - Extrait {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nğŸ“š **Sources utilisÃ©es:**")
    for source in sorted(sources):
        results.append(f"â€¢ {source}")
    
    return "\n".join(results)

@tool
def search_specific_document(filename: str, query: str) -> str:
    """Recherche dans un document spÃ©cifique par nom de fichier."""
    docs = vector_store.similarity_search(
        query, 
        k=5,
        filter={"filename": filename}
    )
    if not docs:
        return f"Aucune information trouvÃ©e dans le document '{filename}' pour cette requÃªte."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"ğŸ“„ {filename} - Section {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nğŸ“š **Sources utilisÃ©es:**")
    for source in sorted(sources):
        results.append(f"â€¢ {source}")
    
    return "\n".join(results)

@tool
def get_document_summary(filename: str) -> str:
    """Obtient un rÃ©sumÃ© d'un document spÃ©cifique."""
    docs = vector_store.similarity_search(
        "", 
        k=10,  # Plus de chunks pour un meilleur rÃ©sumÃ©
        filter={"filename": filename}
    )
    if not docs:
        return f"Document '{filename}' non trouvÃ©."
    
    content = "\n".join([doc.page_content for doc in docs[:5]])  # Premiers 5 chunks
    return f"ğŸ“„ RÃ©sumÃ© de '{filename}':\n{content[:1000]}..."

@tool
def get_database_stats() -> str:
    """Obtient des statistiques sur la base de donnÃ©es documentaire."""
    try:
        # Ã‰chantillonnage pour obtenir des stats
        sample_docs = vector_store.similarity_search("", k=100)
        
        if not sample_docs:
            return "La base de donnÃ©es est vide."
        
        # Compter les documents uniques
        filenames = [doc.metadata.get('filename', 'Inconnu') for doc in sample_docs]
        unique_files = set(filenames)
        
        # Compter les chunks par document
        file_chunks = {}
        for filename in filenames:
            file_chunks[filename] = file_chunks.get(filename, 0) + 1
        
        # CrÃ©er le rapport
        stats = f"ğŸ“Š **Statistiques de la base de donnÃ©es:**\n\n"
        stats += f"â€¢ Nombre total de chunks: {len(sample_docs)}\n"
        stats += f"â€¢ Nombre de documents: {len(unique_files)}\n\n"
        
        stats += "ğŸ“„ **DÃ©tails par document:**\n"
        for filename in sorted(unique_files):
            chunk_count = file_chunks.get(filename, 0)
            stats += f"â€¢ {filename}: {chunk_count} chunks\n"
        
        return stats
    except Exception as e:
        return f"Erreur lors de la rÃ©cupÃ©ration des statistiques: {str(e)}"

@tool
def list_available_documents() -> str:
    """Liste tous les documents disponibles dans la base avec dÃ©tails."""
    try:
        # RÃ©cupÃ©rer un Ã©chantillon de documents pour obtenir les noms de fichiers
        docs = vector_store.similarity_search("", k=50)
        
        if not docs:
            return "Aucun document n'a Ã©tÃ© uploadÃ©."
        
        # Organiser par nom de fichier avec mÃ©tadonnÃ©es
        files_info = {}
        for doc in docs:
            filename = doc.metadata.get('filename', 'Inconnu')
            if filename not in files_info:
                files_info[filename] = {
                    'chunks': 0,
                    'upload_time': doc.metadata.get('upload_time', 'N/A'),
                    'file_size': doc.metadata.get('file_size', 'N/A')
                }
            files_info[filename]['chunks'] += 1
        
        # Formater la liste
        result = "ğŸ“š **Documents disponibles:**\n\n"
        for filename in sorted(files_info.keys()):
            info = files_info[filename]
            result += f"ğŸ“„ **{filename}**\n"
            result += f"   â€¢ Chunks: {info['chunks']}\n"
            if info['upload_time'] != 'N/A':
                # Formatage simplifiÃ© de la date
                try:
                    date_part = info['upload_time'].split('T')[0]
                    result += f"   â€¢ AjoutÃ© le: {date_part}\n"
                except:
                    pass
            result += "\n"
        
        return result
    except Exception as e:
        return f"Erreur: {str(e)}"

@tool
def search_with_context(query: str, context_size: int = 5) -> str:
    """Recherche avec plus de contexte (plus de chunks)."""
    docs = vector_store.similarity_search(query, k=context_size)
    if not docs:
        return "Aucune information trouvÃ©e dans les documents."
    
    results = []
    sources = set()
    for i, doc in enumerate(docs, 1):
        filename = doc.metadata.get('filename', 'Document inconnu')
        sources.add(filename)
        source = f"Source: {filename}"
        results.append(f"ğŸ“„ {filename} - Contexte {i}:\n{doc.page_content}\n{source}\n")
    
    # Add sources summary
    results.append("\nğŸ“š **Sources utilisÃ©es:**")
    for source in sorted(sources):
        results.append(f"â€¢ {source}")
    
    return "\n".join(results)

@tool
def find_related_content(query: str) -> str:
    """Trouve du contenu liÃ©/similaire dans les documents."""
    # PremiÃ¨re recherche
    initial_docs = vector_store.similarity_search(query, k=2)
    if not initial_docs:
        return "Aucun contenu liÃ© trouvÃ©."
    
    # Utiliser le contenu trouvÃ© pour chercher du contenu similaire
    related_results = []
    sources = set()
    for doc in initial_docs:
        # Recherche basÃ©e sur le contenu trouvÃ©
        related_docs = vector_store.similarity_search(doc.page_content[:200], k=2)
        for related_doc in related_docs:
            if related_doc.page_content != doc.page_content:  # Ã‰viter les doublons
                filename = related_doc.metadata.get('filename', 'Document inconnu')
                sources.add(filename)
                source = f"Source: {filename}"
                related_results.append(f"ğŸ“„ {filename} - Contenu liÃ©:\n{related_doc.page_content}\n{source}\n")
    
    if not related_results:
        return "Aucun contenu liÃ© supplÃ©mentaire trouvÃ©."
    
    # Add sources summary
    related_results.append("\nğŸ“š **Sources utilisÃ©es:**")
    for source in sorted(sources):
        related_results.append(f"â€¢ {source}")
    
    return "\n".join(related_results[:3])  # Limiter Ã  3 rÃ©sultats

# Configuration de l'agent avec tous les outils
def create_agent(search_mode="both"):
    llm = init_llm()
    
    # Define tools based on search mode
    tools = []
    if search_mode in ["both", "internal"]:
        tools.extend([
            retrieve,  # Outil principal de retrieve
            search_documents,
            search_specific_document,
            get_document_summary,
            list_available_documents,
            search_with_context,
            find_related_content
        ])
    if search_mode in ["both", "external"]:
        tools.append(search_wikipedia)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Tu es un assistant expert en recherche documentaire. Tu as accÃ¨s Ã  plusieurs outils puissants :

ğŸ” **OUTILS DISPONIBLES** :
1. **retrieve(query)** : Outil principal pour rÃ©cupÃ©rer des informations
2. **search_documents(query)** : Recherche gÃ©nÃ©rale dans tous les documents
3. **search_specific_document(filename, query)** : Recherche dans un document spÃ©cifique
4. **get_document_summary(filename)** : RÃ©sumÃ© d'un document
5. **list_available_documents()** : Liste tous les documents disponibles
6. **search_with_context(query, context_size)** : Recherche avec plus de contexte
7. **find_related_content(query)** : Trouve du contenu connexe

ğŸ“‹ **INSTRUCTIONS** :
- Utilise TOUJOURS au moins un outil avant de rÃ©pondre
- Pour les questions gÃ©nÃ©rales â†’ retrieve() ou search_documents()
- Pour chercher dans un fichier spÃ©cifique â†’ search_specific_document()
- Pour lister les fichiers â†’ list_available_documents()
- Pour plus de dÃ©tails â†’ search_with_context()
- RÃ©ponds UNIQUEMENT avec les informations trouvÃ©es

ğŸ’¡ **EXEMPLES D'USAGE** :
- "Dis-moi ce que contiennent les documents" â†’ retrieve("contenu")
- "RÃ©sume le document rapport.pdf" â†’ get_document_summary("rapport.pdf")
- "Cherche dans document.pdf les infos sur X" â†’ search_specific_document("document.pdf", "X")

ğŸš« **RÃˆGLES STRICTES** :
- Jamais de connaissances gÃ©nÃ©rales
- Toujours utiliser les outils
- Si rien trouvÃ© : "Aucune information sur ce sujet dans les documents."
"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    agent = create_tool_calling_agent(llm, tools, prompt)
    return AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        max_iterations=3,
        handle_parsing_errors=True
    )

# Fonction pour vÃ©rifier si un document existe dÃ©jÃ 
def check_document_exists(filename):
    """VÃ©rifie si un document est dÃ©jÃ  dans la base vectorielle."""
    try:
        # Rechercher des documents avec ce nom de fichier
        docs = vector_store.similarity_search(
            "", 
            k=10,
            filter={"filename": filename}
        )
        return len(docs) > 0
    except Exception as e:
        return False

# Fonction pour traiter les fichiers (avec gestion des gros fichiers)
def process_file(uploaded_file):
    """Traite et indexe un fichier uploadÃ© (Ã©vite les doublons et gÃ¨re les gros fichiers)."""
    
    # VÃ©rifier si le document existe dÃ©jÃ 
    if check_document_exists(uploaded_file.name):
        return "exists"
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    # Charger le document
    if uploaded_file.name.endswith('.pdf'):
        loader = PyPDFLoader(tmp_path)
    elif uploaded_file.name.endswith('.txt'):
        loader = TextLoader(tmp_path)
    elif uploaded_file.name.endswith('.docx'):
        loader = Docx2txtLoader(tmp_path)
    else:
        os.unlink(tmp_path)
        return False
    
    docs = loader.load()
    os.unlink(tmp_path)
    
    # Diviser en chunks Ã©quilibrÃ©s
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # Taille Ã©quilibrÃ©e
        chunk_overlap=100,   # Bon overlap pour la cohÃ©rence
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(docs)
    
    # Ajouter metadata avec timestamp pour traÃ§abilitÃ©
    import datetime
    current_time = datetime.datetime.now().isoformat()
    
    for i, chunk in enumerate(chunks):
        chunk.metadata.update({
            'filename': uploaded_file.name,
            'chunk_index': i,
            'total_chunks': len(chunks),
            'upload_time': current_time,
            'file_size': len(uploaded_file.getvalue())
        })
    
    # Indexer par batches pour Ã©viter les erreurs de taille
    batch_size = 50  # Traiter 50 chunks Ã  la fois
    total_batches = (len(chunks) + batch_size - 1) // batch_size
    
    try:
        progress_text = st.empty()
        progress_bar = st.progress(0)
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            current_batch = (i // batch_size) + 1
            
            # Mettre Ã  jour l'interface
            progress_text.text(f"â³ Indexation batch {current_batch}/{total_batches} ({len(batch)} chunks)")
            progress_bar.progress(current_batch / total_batches)
            
            # Ajouter le batch Ã  Pinecone
            vector_store.add_documents(batch)
            
            # Pause courte pour Ã©viter la surcharge
            import time
            time.sleep(0.5)
        
        # Nettoyer l'interface
        progress_text.empty()
        progress_bar.empty()
        
        return len(chunks)
        
    except Exception as e:
        st.error(f"Erreur lors de l'indexation: {str(e)}")
        return False

# Interface utilisateur amÃ©liorÃ©e
st.title("ğŸ¤– Assistant RAG Multilingue (GPT-4o Mini)")
st.caption("ğŸŒ Posez vos questions en franÃ§ais, anglais, espagnol ou toute autre langue")
st.caption("âš¡ Powered by GPT-4o Mini - Rapide et efficace")

# Sidebar pour upload avec gestion des doublons
with st.sidebar:
    st.header("ğŸ“ Upload de Documents")
    
    # Afficher le statut de la base de donnÃ©es avec gestion des erreurs
    try:
        # Obtenir quelques stats sur les documents existants
        sample_docs = vector_store.similarity_search("", k=20)
        existing_files = set([doc.metadata.get('filename', 'Inconnu') for doc in sample_docs])
        
        if existing_files:
            st.success(f"âœ… Connexion Pinecone active")
            st.info(f"ğŸ“š {len(existing_files)} document(s) dÃ©jÃ  dans la base")
            with st.expander("Voir les documents existants"):
                for filename in sorted(existing_files):
                    st.write(f"ğŸ“„ {filename}")
        else:
            st.info("ğŸ“‚ Base de donnÃ©es vide - PrÃªte pour le premier document")
    except Exception as e:
        st.error(f"âš ï¸ ProblÃ¨me de connexion Pinecone: {str(e)}")
        st.info("ğŸ’¡ VÃ©rifiez votre clÃ© API et la configuration de l'index")
    
    uploaded_file = st.file_uploader(
        "Choisir un fichier",
        type=['pdf', 'txt', 'docx']
    )
    
    if uploaded_file:
        # Afficher des informations sur le fichier
        file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.write(f"ğŸ“ Fichier: {uploaded_file.name}")
        st.write(f"ğŸ“ Taille: {file_size_mb:.2f} MB")
        
        if file_size_mb > 5:
            st.warning("âš ï¸ Fichier volumineux - Le traitement peut prendre du temps")
        
        with st.spinner("â³ VÃ©rification et traitement..."):
            result = process_file(uploaded_file)
            
            if result == "exists":
                st.warning(f"âš ï¸ Le document '{uploaded_file.name}' existe dÃ©jÃ  dans la base !")
                st.info("ğŸ’¡ Pas besoin de le reprocesser. Vous pouvez directement poser des questions.")
            elif result and isinstance(result, int):
                st.success(f"âœ… Nouveau document ajoutÃ© ! {result} chunks indexÃ©s pour '{uploaded_file.name}'")
                st.balloons()  # Animation de cÃ©lÃ©bration
            elif result == False:
                st.error("âŒ Type de fichier non supportÃ© ou erreur de traitement")
            else:
                st.error("âŒ Erreur lors du traitement")
    
    # Bouton pour effacer la base (optionnel)
    st.divider()
    if st.button("ğŸ—‘ï¸ Effacer la base de donnÃ©es", type="secondary"):
        if st.checkbox("Confirmer la suppression"):
            try:
                # Ici on peut supprimer l'index et le recrÃ©er
                # Ou utiliser d'autres mÃ©thodes selon votre configuration Pinecone
                st.warning("âš ï¸ FonctionnalitÃ© de suppression Ã  implÃ©menter si nÃ©cessaire")
            except Exception as e:
                st.error(f"Erreur: {e}")
    
    # Section d'aide
    st.header("ğŸ’¡ Exemples de Questions")
    st.markdown("""
    **ğŸŒ L'assistant rÃ©pond dans votre langue :**
    
    **ğŸ“„ Recherche Interne (Documents):**
    - "Quels documents sont disponibles ?"
    - "What documents are available?"
    - "RÃ©sume le document rapport.pdf"
    - "Search for information about [topic] in my documents"
    
    **ğŸŒ Recherche Externe (Wikipedia):**
    - "Recherche sur Wikipedia l'intelligence artificielle"
    - "Search Wikipedia for machine learning"
    - "Â¿QuÃ© dice Wikipedia sobre blockchain?"
    - "Find information about quantum computing on Wikipedia"
    
    **ğŸ”„ Recherche CombinÃ©e (Les Deux):**
    - "Compare mes documents avec les infos Wikipedia sur [sujet]"
    - "What do my documents say about AI vs Wikipedia?"
    - "InformaciÃ³n interna y externa sobre [tema]"
    
    **ğŸ” Recherche avancÃ©e :**
    - "Search with more context on [topic]"
    - "Cherche du contenu liÃ© Ã  [sujet]"
    - "Statistiques de la base de donnÃ©es"
    """)
    
    # Indicateur du mode actuel
    current_mode = st.session_state.get("search_mode", "both")
    if current_mode == "both":
        st.success("ğŸ”„ Mode Hybride: Documents + Wikipedia")
    elif current_mode == "internal":
        st.info("ğŸ“„ Mode Interne: Documents uniquement")
    else:
        st.info("ğŸŒ Mode Externe: Wikipedia uniquement")

# Zone principale avec sÃ©lecteur de mode
col1, col2 = st.columns([3, 1])

with col2:
    st.subheader("ğŸ” Mode de Recherche")
    # Initialiser search_mode dans session_state s'il n'existe pas
    if "search_mode" not in st.session_state:
        st.session_state.search_mode = "both"
    
    search_mode = st.radio(
        "Choisissez le type de recherche:",
        ["both", "internal", "external"],
        format_func=lambda x: {
            "both": "ğŸ”„ Les Deux (Documents + Wikipedia)",
            "internal": "ğŸ“„ Documents UploadÃ©s Seulement", 
            "external": "ğŸŒ Wikipedia Seulement"
        }[x],
        index=0,
        key="search_mode_radio"
    )
    
    # Mettre Ã  jour session_state
    st.session_state.search_mode = search_mode
    
    st.subheader("ğŸ› ï¸ Outils Disponibles")
    current_mode = st.session_state.get("search_mode", "both")
    if current_mode in ["internal", "both"]:
        st.write("""
        **ğŸ“„ Outils Internes:**
        - ğŸ” Recherche gÃ©nÃ©rale
        - ğŸ“„ Recherche spÃ©cifique
        - ğŸ“‹ RÃ©sumÃ© de document
        - ğŸ“š Liste des documents
        - ğŸ” Recherche contextuelle
        - ğŸ”— Contenu connexe
        - ğŸ“Š Statistiques
        """)
    
    if current_mode in ["external", "both"]:
        st.write("""
        **ğŸŒ Outils Externes:**
        - ğŸ“š Recherche Wikipedia
        """)
    
    st.info(f"Mode actuel: {current_mode.upper()}")
    st.caption("ğŸŒ L'assistant s'adapte automatiquement Ã  la langue de votre question")

with col1:
    # Chat interface
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Afficher l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # Input utilisateur avec traitement direct de la langue
    if prompt := st.chat_input("Posez votre question... | Ask your question..."):
        # Ajouter le message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        # DÃ©tecter la langue et crÃ©er une instruction explicite
        def is_english(text):
            english_words = ['what', 'how', 'where', 'when', 'why', 'can', 'could', 'would', 'should', 'list', 'show', 'tell', 'explain', 'describe', 'the', 'for', 'tools', 'models', 'used']
            return any(word.lower() in text.lower() for word in english_words)
        
        def is_french(text):
            french_words = ['que', 'quoi', 'comment', 'oÃ¹', 'quand', 'pourquoi', 'peux', 'pourrais', 'devrait', 'liste', 'montre', 'dis', 'explique', 'dÃ©cris', 'les', 'pour', 'outils', 'modÃ¨les', 'utilisÃ©s']
            return any(word.lower() in text.lower() for word in french_words)
        
        # CrÃ©er un prompt avec instruction de langue explicite
        if is_english(prompt):
            enhanced_prompt = f"INSTRUCTION: Respond in English only.\n\nUser question: {prompt}"
        elif is_french(prompt):
            enhanced_prompt = f"INSTRUCTION: RÃ©pondre en franÃ§ais uniquement.\n\nQuestion utilisateur: {prompt}"
        else:
            enhanced_prompt = f"INSTRUCTION: Respond in the same language as this question.\n\nUser question: {prompt}"
        
        # GÃ©nÃ©rer la rÃ©ponse avec le mode sÃ©lectionnÃ©
        current_search_mode = st.session_state.get("search_mode", "both")
        agent = create_agent(search_mode=current_search_mode)
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Analyse en cours... | Analysis in progress..."):
                try:
                    # PrÃ©parer l'historique pour l'agent
                    chat_history = []
                    for msg in st.session_state.messages[:-1]:  # Exclure le dernier message
                        if msg["role"] == "user":
                            chat_history.append(HumanMessage(content=msg["content"]))
                        else:
                            chat_history.append(AIMessage(content=msg["content"]))
                    
                    # Ajouter l'information du mode de recherche au prompt
                    mode_info = {
                        "both": "You can search in uploaded documents AND Wikipedia. Use both sources when relevant.",
                        "internal": "You can ONLY search in uploaded documents. Do not use external knowledge.",
                        "external": "You can ONLY search on Wikipedia. Do not refer to uploaded documents."
                    }
                    
                    enhanced_prompt = f"{enhanced_prompt}\n\nSEARCH MODE: {current_search_mode.upper()} - {mode_info[current_search_mode]}"
                    
                    # ExÃ©cuter l'agent avec l'instruction de langue et mode
                    response = agent.invoke({
                        "input": enhanced_prompt,
                        "chat_history": chat_history
                    })
                    
                    answer = response['output']
                    
                    # Post-traitement pour vÃ©rifier la langue (en cas d'Ã©chec)
                    if is_english(prompt) and not any(word in answer.lower() for word in ['the', 'are', 'is', 'and', 'tools', 'documents']):
                        # Si la rÃ©ponse semble Ãªtre en franÃ§ais alors que la question Ã©tait en anglais
                        st.warning("ğŸ”„ Correction automatique de la langue...")
                        
                        # Nouvelle tentative avec instruction plus forte
                        second_prompt = f"You MUST respond in ENGLISH. Translate and reformat this answer to English: {answer}"
                        second_response = agent.invoke({
                            "input": second_prompt,
                            "chat_history": []
                        })
                        answer = second_response['output']
                    
                    st.write(answer)
                    
                    # Afficher les outils utilisÃ©s
                    if 'intermediate_steps' in response:
                        with st.expander("ğŸ”§ Outils utilisÃ©s | Tools used"):
                            for step in response['intermediate_steps']:
                                tool_name = step[0].tool
                                tool_input = step[0].tool_input
                                st.write(f"**{tool_name}** : {tool_input}")
                    
                    # Ajouter Ã  l'historique
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    error_msg = f"Erreur | Error: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

# Bouton pour vider l'historique
if st.sidebar.button("ğŸ—‘ï¸ Vider l'historique"):
    st.session_state.messages = []
    st.rerun()